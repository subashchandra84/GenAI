# -*- coding: utf-8 -*-
"""Fine_tune_Llama_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bd7c5rioBOmtJbDEax83vAHEPru-r06l

#**Step 1: Install All the Required Packages**
"""

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

"""#**Step 2: Import All the Required Libraries**"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

"""#**In case of Llama 2, the following prompt template is used for the chat models**

System Prompt (optional) to guide the model


User prompt (required) to give the instruction


Model Answer (required)

![image.png](data:image/png;base64,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)

#We will reformat our instruction dataset to follow Llama 2 template.

- Orignal Dataset: https://huggingface.co/datasets/timdettmers/openassistant-guanaco

- Reformat Dataset following the Llama 2 template with 1k sample: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k

- Complete Reformat Dataset following the Llama 2 template: https://huggingface.co/datasets/mlabonne/guanaco-llama2

To know how this dataset was created, you can check this notebook.  

https://colab.research.google.com/drive/1Ad7a9zMmkxuXTOh1Z7-rNSICA4dybpM2?usp=sharing

### Note: You don’t need to follow a specific prompt template if you’re using the base Llama 2 model instead of the chat version.

#**How to fine tune Llama 2**

- Free Google Colab offers a 15GB Graphics Card (Limited Resources --> Barely enough to store Llama 2–7b’s weights)

- We also need to consider the overhead due to optimizer states, gradients, and forward activations

- Full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.

- To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here.

#**Step 3**

1. Load a llama-2-7b-chat-hf model (chat model)
2. Train it on the mlabonne/guanaco-llama2-1k (1,000 samples), which will produce our fine-tuned model Llama-2-7b-chat-finetune

QLoRA will use a rank of 64 with a scaling parameter of 16. We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch
"""

# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"

# The instruction dataset to use
dataset_name = "mlabonne/guanaco-llama2-1k"

# Fine-tuned model name
new_model = "Llama-2-7b-chat-finetune"

################################################################################
# QLoRA parameters
################################################################################

# LoRA attention dimension
lora_r = 64

# Alpha parameter for LoRA scaling
lora_alpha = 16

# Dropout probability for LoRA layers
lora_dropout = 0.1

################################################################################
# bitsandbytes parameters
################################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

################################################################################
# TrainingArguments parameters
################################################################################

# Output directory where the model predictions and checkpoints will be stored
output_dir = "./results"

# Number of training epochs
num_train_epochs = 1

# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16 = False
bf16 = False

# Batch size per GPU for training
per_device_train_batch_size = 4

# Batch size per GPU for evaluation
per_device_eval_batch_size = 4

# Number of update steps to accumulate the gradients for
gradient_accumulation_steps = 1

# Enable gradient checkpointing
gradient_checkpointing = True

# Maximum gradient normal (gradient clipping)
max_grad_norm = 0.3

# Initial learning rate (AdamW optimizer)
learning_rate = 2e-4

# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay = 0.001

# Optimizer to use
optim = "paged_adamw_32bit"

# Learning rate schedule
lr_scheduler_type = "cosine"

# Number of training steps (overrides num_train_epochs)
max_steps = -1

# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03

# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = True

# Save checkpoint every X updates steps
save_steps = 0

# Log every X updates steps
logging_steps = 25

################################################################################
# SFT parameters
################################################################################

# Maximum sequence length to use
max_seq_length = None

# Pack multiple short examples in the same input sequence to increase efficiency
packing = False

# Load the entire model on the GPU 0
device_map = {"": 0}

"""#**Step 4:Load everything and start the fine-tuning process**

1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.


2. Then, we’re configuring bitsandbytes for 4-bit quantization.


3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.


4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!
"""

# Load dataset (you can process it here)
dataset = load_dataset(dataset_name, split="train")

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)

"""##**Step 5: Check the plots on tensorboard, as follows**"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir results/runs

"""###**Step 6:Use the text generation pipeline to ask questions like “What is a large language model?” Note that I’m formatting the input to match Llama 2 prompt template.**"""

# Ignore warnings
logging.set_verbosity(logging.CRITICAL)

# Run text generation pipeline with our next model
prompt = "What is a large language model?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])

# Empty VRAM
del model
del pipe
del trainer
import gc
gc.collect()
gc.collect()

"""You can train a Llama 2 model on the entire dataset using [mlabonne/guanaco-llama2](https://huggingface.co/datasets/mlabonne/guanaco-llama2)

#**Step 7: Store New Llama2 Model (Llama-2-7b-chat-finetune)**
"""



"""How can we store our new Llama-2-7b-chat-finetune model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything."""

# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)
model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()

# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""#**Step 8: Push Model to Hugging Face Hub**

Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!huggingface-cli login

model.push_to_hub("entbappy/Llama-2-7b-chat-finetune", check_pr=True)

tokenizer.push_to_hub("entbappy/Llama-2-7b-chat-finetune",check_pr=True)

"""You can now use this model for inference by loading it like any other Llama 2 model from the Hub."""

